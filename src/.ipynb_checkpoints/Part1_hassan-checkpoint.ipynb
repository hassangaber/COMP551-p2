{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00ee38e",
   "metadata": {},
   "source": [
    "# Part 1.1 - Optimizing test accuracy as a function of learning rate and maximum iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b19e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing standard modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lr\n",
    "import lr_m        # momentum version\n",
    "\n",
    "# SKlearn\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f84598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing csv data\n",
    "train = pd.read_csv('../data/diabetes/diabetes_train.csv')\n",
    "test = pd.read_csv('../data/diabetes/diabetes_test.csv')\n",
    "valid = pd.read_csv('../data/diabetes/diabetes_val.csv')\n",
    "all_data = pd.concat([train, test, valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2b0bcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to find convergent solution of gradient descent as a function of learning-rate and maximum iterations\n",
    "\n",
    "def optimize(data, max_iter, learning_rate, pred_data):\n",
    "    \"\"\"\n",
    "    Optimize a logisitic regression model's weights using gradient descent.\n",
    "    This function makes it easier to test different combinations of the\n",
    "    maximum iterations and learning rate parameters. \n",
    "    data:           [pd.DataFrame]  Training or Training+Validation data\n",
    "    max_iter:       [int]           Maximum Iterations of gradient descent\n",
    "    learning_rate:  [float]\n",
    "    pred_data:      [pd.DataFrame]\n",
    "    returns array of class probabilities\n",
    "    \"\"\"\n",
    "    A = []\n",
    "\n",
    "    # Input data, features and binary labels column\n",
    "    Xin = data.drop('Outcome',axis=1).to_numpy()\n",
    "    Yin = data['Outcome'].to_numpy()\n",
    "    \n",
    "    # Prediction data, validation set\n",
    "    Xp = pred_data.drop('Outcome',axis=1).to_numpy()\n",
    "    Yp = pred_data['Outcome'].to_numpy().ravel()\n",
    "    \n",
    "    # Iterate through the input parameters\n",
    "    for l in learning_rate:\n",
    "        \n",
    "        for m in max_iter:\n",
    "            \n",
    "            print(f'LEARNING RATE: {l} \\n')\n",
    "            \n",
    "            model = lr.LogisticRegression(verbose=True, add_bias=True, learning_rate=l, max_iters=m)\n",
    "            yh = model.fit(Xin,Yin).predict(Xp)\n",
    "\n",
    "            T = []\n",
    "            T.append(yh)\n",
    "        \n",
    "            # Decision Boundary\n",
    "            prediction = []\n",
    "            for x in np.array(T).ravel():\n",
    "                if x < 0.5: prediction.append(0)\n",
    "                else: prediction.append(1)\n",
    "\n",
    "            #print(T)\n",
    "            #print(prediction)\n",
    "            #print(Yp)\n",
    "            A.append(accuracy_score(Yp,np.array(prediction).ravel()))\n",
    "            print(\"Accuracy Score:\", accuracy_score(Yp,np.array(prediction).ravel()))\n",
    "    \n",
    "    print(f'Maximum Accuracy achieved: {max(A)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e418286",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE: 0.0002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 0.013429351710347736\n",
      "\n",
      "Weights: [ 1.29152029e-01  2.50264215e-02 -1.86834650e-02 -2.52562870e-03\n",
      "  4.12691159e-04  4.13164885e-02  5.23867127e-01  6.70382786e-04\n",
      " -4.46242101e+00]\n",
      "\n",
      "Accuracy Score: 0.7794117647058824\n",
      "Maximum Accuracy achieved: 0.7794117647058824\n"
     ]
    }
   ],
   "source": [
    "# Best set of parameters so far:\n",
    "m = [1e6] #3e5]\n",
    "l = [2e-4]\n",
    "\n",
    "optimize(data=train, max_iter=m, learning_rate=l, pred_data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f941720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function determines the accuracy the Sklearn logisitic classifier can achieve\n",
    "\n",
    "def compareSK(pred_data):\n",
    "    \n",
    "    # Input data, features and binary labels column\n",
    "    Xin = train.drop('Outcome',axis=1).to_numpy()\n",
    "    Yin = train['Outcome'].to_numpy()\n",
    "    \n",
    "    # Prediction data, validation set\n",
    "    Xp = pred_data.drop('Outcome',axis=1).to_numpy()\n",
    "    Yp = pred_data['Outcome'].to_numpy().ravel()\n",
    "    \n",
    "    model = SGDClassifier(max_iter=1e6, alpha=0.0002)\n",
    "    yh = model.fit(Xin,Yin).predict(Xp)\n",
    "    \n",
    "    T = []\n",
    "    T.append(yh)\n",
    "    print(T)\n",
    "    \n",
    "    # Decision Boundary\n",
    "    prediction = []\n",
    "    for x in np.array(T).ravel():\n",
    "        if x < 0.5: prediction.append(0)\n",
    "        else: prediction.append(1)\n",
    "            \n",
    "    print(Yp)\n",
    "    \n",
    "    print(\"Accuracy Score:\", accuracy_score(Yp,np.array(prediction).ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "168c779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0])]\n",
      "[0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1]\n",
      "Accuracy Score: 0.6911764705882353\n"
     ]
    }
   ],
   "source": [
    "compareSK(pred_data=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6558c",
   "metadata": {},
   "source": [
    "## Results & Remarks\n",
    "\n",
    "* Our base model preformed better than the Sklearn SGD logisitic classifier with a margin greater than 10%\n",
    "\n",
    "* Decision Boundary is set at X=0.5: $$P(y=1|X) < 0.5 \\rightarrow \\hat{y}=0$$ $$P(y=1|X) \\geq 0.5 \\rightarrow \\hat{y}= 1$$ \n",
    "\n",
    "\n",
    "* Best accuracy achieved on the test set was 77.9% with $\\alpha = 2 * 10^{-4}$ and $i_{max} = 1 * 10^6$\n",
    "\n",
    "* Output (including weights and magnitude of gradient vector):\n",
    "\n",
    "```\n",
    "                    LEARNING RATE: 2e-4\n",
    "                    \n",
    "                    1000000 Iterations\n",
    "                    Norm of gradient: 0.013429351710347736\n",
    "\n",
    "                    Weights: [ 1.29152029e-01  2.50264215e-02 -1.86834650e-02 -2.52562870e-03\n",
    "                      4.12691159e-04  4.13164885e-02  5.23867127e-01  6.70382786e-04\n",
    "                     -4.46242101e+00]\n",
    "\n",
    "                    Accuracy Score: 0.7794117647058824\n",
    "                    \n",
    "                    -----------------------------------------------------------------------------\n",
    "                    \n",
    "                    LEARNING RATE: 9e-05 \n",
    "\n",
    "                    5000000 Iterations\n",
    "                    Norm of gradient: 0.005435090157130823\n",
    "\n",
    "                    Weights: [ 1.36309194e-01  3.11128368e-02 -1.48346749e-02 -3.04513818e-03\n",
    "                     -8.58120779e-05  6.64128026e-02  7.09122848e-01  6.22651319e-03\n",
    "                     -6.61628338e+00]\n",
    "\n",
    "                    Accuracy Score: 0.7794117647058824\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef3bc1",
   "metadata": {},
   "source": [
    "# Part 1.2 - Implementing mini-batch stochastic gradient descent\n",
    "\n",
    "- Mini-batch gradient descent relies on splitting the training data into batches and running the gradient descent algorithm on each set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be506447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchData(data: pd.DataFrame, size: int, b=0, with_num=False, split=False):\n",
    "    \"\"\"\n",
    "    Splits the data into specific batches. Allows user to batch by features and labels\n",
    "    or batch the entire dataset. Also, allows the user to have a certain number of batches\n",
    "    and not batch the entire data set.\n",
    "    \n",
    "    data:     [pd.DataFrame]  This is the training data as input\n",
    "    size:     [int]           Size of batches in units of data points\n",
    "    b:        [int]           Indicates how many batches of the data will be returned\n",
    "    with_num: [bool]          Lets user decide how many batches it should generate\n",
    "    \n",
    "    returns batched data\n",
    "    \"\"\"\n",
    "    if split:\n",
    "        # Split the data into features and labels\n",
    "        X = data.drop('Outcome', axis=1)\n",
    "        Y = data['Outcome']\n",
    "        \n",
    "        # Iterate through data and split it based on batch size and number of batches needed\n",
    "        # Function can handle iterating through the entire dataset or for certain number of batches\n",
    "        if with_num:\n",
    "            for x in range(0, b*size, size):\n",
    "                yield X[x : min(x + size, b*size)], Y[x : min(x + size, b*size)]\n",
    "\n",
    "        else:      \n",
    "            for x in range(0, len(data), size):\n",
    "                yield X[x : min(x + size, len(data))], Y[x : min(x + size, len(data))]\n",
    "                           \n",
    "    else:\n",
    "        if with_num:\n",
    "            for x in range(0, b*size, size):\n",
    "                yield data[x : min(x + size, b*size)]\n",
    "\n",
    "        else:      \n",
    "            for x in range(0, len(data), size):\n",
    "                yield data[x : min(x + size, len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91e87e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test batching for entire data set\n",
    "for x in BatchData(train, 2):\n",
    "    print(f'{x}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc865a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batching for discrete batches\n",
    "for batch in BatchData(train, 32,split=False):\n",
    "    print(f'{batch}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429f79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miniBatchGD(data, BatchSize, epochs=1, All_data=True, NumBatches=0):\n",
    "    \"\"\"\n",
    "    Run gradient descent optimization in mini-batches\n",
    "    \n",
    "    data:       [pd.DataFrame]  Input data\n",
    "    BatchSize:  [int]           Size of mini-batch\n",
    "    epoch:      [int]           Number of iterations over the batches\n",
    "    NumBatches: [int]           How many batches it should generate\n",
    "    All_data:   [bool]          If the user wants to batch the entire dataset\n",
    "    \n",
    "    returns weights and accuracy on testing set with mini-batch\n",
    "    \n",
    "    \"\"\"\n",
    "    if All_data:\n",
    "        for epoch in range(1, epochs+1):\n",
    "            data = data.sample(frac=1)\n",
    "            print(f'Epoch: {epoch}')\n",
    "            for batch in BatchData(data, BatchSize, split=False):\n",
    "                optimize(data=batch, max_iter=[1e6], learning_rate=[2e-4], pred_data=test)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7bf9e0",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "* Mini-batch sizes < 75 rows usually yield lower accuracies than batch sizes > 75 rows\n",
    "* Large mini-batch sizes outpreform full gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ee82c40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "LEARNING RATE: 0.0002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 0.015892992183576222\n",
      "\n",
      "Weights: [ 1.20403970e-02  2.95064336e-02 -3.56167999e-02  2.06340238e-02\n",
      "  1.25484151e-03  4.49286155e-02  4.90184967e-01  2.36078082e-02\n",
      " -5.09488582e+00]\n",
      "\n",
      "Accuracy Score: 0.7205882352941176\n",
      "Maximum Accuracy achieved: 0.7205882352941176\n",
      "LEARNING RATE: 0.0002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 0.014758168905036687\n",
      "\n",
      "Weights: [ 1.78509158e-01  1.75080633e-02 -1.82363998e-02 -1.79815919e-02\n",
      "  1.13944485e-03  8.77708862e-02  4.35963845e-01 -2.97611228e-03\n",
      " -4.77279942e+00]\n",
      "\n",
      "Accuracy Score: 0.7941176470588235\n",
      "Maximum Accuracy achieved: 0.7941176470588235\n",
      "LEARNING RATE: 0.0002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 0.01532299558079225\n",
      "\n",
      "Weights: [ 2.17363378e-01  2.67336828e-02 -2.87669803e-03 -1.93292832e-02\n",
      "  2.86537496e-03  1.62444479e-02  1.32195098e+00 -2.98533678e-02\n",
      " -4.46492917e+00]\n",
      "\n",
      "Accuracy Score: 0.7352941176470589\n",
      "Maximum Accuracy achieved: 0.7352941176470589\n",
      "LEARNING RATE: 0.0002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 0.010371863641820855\n",
      "\n",
      "Weights: [ 6.46701656e-02  2.52435318e-02 -2.31947306e-02  9.05243113e-03\n",
      " -1.78169991e-03 -3.90061102e-03  3.72834946e-01  2.54319499e-02\n",
      " -3.35470930e+00]\n",
      "\n",
      "Accuracy Score: 0.7647058823529411\n",
      "Maximum Accuracy achieved: 0.7647058823529411\n",
      "Epoch: 2\n",
      "LEARNING RATE: 0.0002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 0.016479202407406048\n",
      "\n",
      "Weights: [ 1.40467204e-01  2.34649031e-02 -1.20353633e-02  8.01903195e-04\n",
      " -8.52750675e-04  5.39446188e-02  4.65446000e-01  6.82655018e-03\n",
      " -5.23669353e+00]\n",
      "\n",
      "Accuracy Score: 0.7941176470588235\n",
      "Maximum Accuracy achieved: 0.7941176470588235\n",
      "LEARNING RATE: 0.0002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 0.013495678699803856\n",
      "\n",
      "Weights: [ 2.98575559e-01  3.02186613e-02 -1.34118855e-02  4.48991955e-03\n",
      "  3.71412547e-03  8.84571697e-03  4.40150084e-01 -3.47516620e-02\n",
      " -4.26442671e+00]\n",
      "\n",
      "Accuracy Score: 0.7058823529411765\n",
      "Maximum Accuracy achieved: 0.7058823529411765\n",
      "LEARNING RATE: 0.0002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 0.01231016330639039\n",
      "\n",
      "Weights: [-1.47842579e-02  1.95579112e-02 -2.58334263e-02 -3.14273468e-03\n",
      " -2.95438193e-03  3.67827399e-02  7.56813805e-01  5.34146049e-02\n",
      " -4.01719813e+00]\n",
      "\n",
      "Accuracy Score: 0.7794117647058824\n",
      "Maximum Accuracy achieved: 0.7794117647058824\n",
      "LEARNING RATE: 0.0002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 0.01529158720566278\n",
      "\n",
      "Weights: [ 1.55732352e-01  2.99555697e-02 -3.22671802e-02 -1.56445792e-02\n",
      "  1.92451380e-03  6.57379922e-02  8.86383616e-01 -2.61648656e-02\n",
      " -4.50301047e+00]\n",
      "\n",
      "Accuracy Score: 0.75\n",
      "Maximum Accuracy achieved: 0.75\n"
     ]
    }
   ],
   "source": [
    "miniBatchGD(data=train, BatchSize=150, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a7620",
   "metadata": {},
   "source": [
    "# Part 1.3: Momentum Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc957152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find convergent solution of gradient descent as a function of learning-rate and maximum iterations\n",
    "\n",
    "def optimize_Momentum(data, max_iter, learning_rate, pred_data, momentum):\n",
    "    \"\"\"\n",
    "    Optimize a logisitic regression model's weights using Momentum gradient descent.\n",
    "    This function makes it easier to test different combinations of the\n",
    "    maximum iterations and learning rate parameters. \n",
    "    data:           [pd.DataFrame]  Training or Training+Validation data\n",
    "    max_iter:       [int]           Maximum Iterations of gradient descent\n",
    "    learning_rate:  [float]\n",
    "    pred_data:      [pd.DataFrame]\n",
    "    momentum:       [int]\n",
    "    returns array of class probabilities\n",
    "    \"\"\"\n",
    "    A = []\n",
    "\n",
    "    # Input data, features and binary labels column\n",
    "    Xin = data.drop('Outcome',axis=1).to_numpy()\n",
    "    Yin = data['Outcome'].to_numpy()\n",
    "    \n",
    "    # Prediction data, validation set\n",
    "    Xp = pred_data.drop('Outcome',axis=1).to_numpy()\n",
    "    Yp = pred_data['Outcome'].to_numpy().ravel()\n",
    "    \n",
    "    # Iterate through the input parameters\n",
    "    for l in learning_rate:\n",
    "        \n",
    "        for z in max_iter:\n",
    "            \n",
    "            print(f'LEARNING RATE: {l} \\n')\n",
    "            \n",
    "            model = lr_m.LogisticRegression(verbose=True, add_bias=True, learning_rate=l, max_iters=z, momentum=momentum)\n",
    "            yh = model.fit(Xin,Yin).predict(Xp)\n",
    "\n",
    "            T = []\n",
    "            T.append(yh)\n",
    "        \n",
    "            # Decision Boundary\n",
    "            prediction = []\n",
    "            for x in np.array(T).ravel():\n",
    "                if x < 0.5: prediction.append(0)\n",
    "                else: prediction.append(1)\n",
    "\n",
    "            #print(T)\n",
    "            #print(prediction)\n",
    "            #print(Yp)\n",
    "            A.append(accuracy_score(Yp,np.array(prediction).ravel()))\n",
    "            print(\"Accuracy Score:\", accuracy_score(Yp,np.array(prediction).ravel()))\n",
    "    \n",
    "    print(f'Maximum Accuracy achieved: {max(A)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e48be63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum value: 0.0\n",
      "-----------------------------------------------------------------------------------\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 57.63983246854505\n",
      "\n",
      "Weights: [ 1.04850422e+00  2.54674993e-01 -7.98565076e-02 -2.87118401e-02\n",
      "  5.14672817e-02  3.20682686e-01  4.66941524e+00  4.35855919e-02\n",
      " -4.01096719e+01]\n",
      "\n",
      "Accuracy Score: 0.5882352941176471\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "5000000 Iterations\n",
      "Norm of gradient: 49.79002716281724\n",
      "\n",
      "Weights: [ 9.60926472e-01  2.54100178e-01 -4.91331041e-02 -3.77763898e-02\n",
      "  4.25735225e-02  5.12856204e-01  4.65508263e+00  7.35102628e-02\n",
      " -4.99702377e+01]\n",
      "\n",
      "Accuracy Score: 0.6176470588235294\n",
      "Maximum Accuracy achieved: 0.6176470588235294\n",
      "Momentum value: 0.1\n",
      "-----------------------------------------------------------------------------------\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 44.769349655508634\n",
      "\n",
      "Weights: [ 8.25065244e-01  1.47326359e-01 -8.94335489e-02 -3.63719547e-02\n",
      " -1.24739929e-02  2.94772782e-01  3.94536449e+00  2.98567217e-02\n",
      " -3.54430170e+01]\n",
      "\n",
      "Accuracy Score: 0.7058823529411765\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "5000000 Iterations\n",
      "Norm of gradient: 40.39107461354911\n",
      "\n",
      "Weights: [ 7.76094131e-01  1.52706181e-01 -6.90316337e-02 -3.99442184e-02\n",
      " -1.34062832e-02  4.03835619e-01  3.82192764e+00  4.71880247e-02\n",
      " -4.07330382e+01]\n",
      "\n",
      "Accuracy Score: 0.7205882352941176\n",
      "Maximum Accuracy achieved: 0.7205882352941176\n",
      "Momentum value: 0.2\n",
      "-----------------------------------------------------------------------------------\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 34.78757926443204\n",
      "\n",
      "Weights: [ 6.51532041e-01  1.20808173e-01 -6.64693079e-02 -3.10728707e-02\n",
      " -1.04834482e-02  2.72512598e-01  3.19778320e+00  3.05090553e-02\n",
      " -3.03234435e+01]\n",
      "\n",
      "Accuracy Score: 0.7058823529411765\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "5000000 Iterations\n",
      "Norm of gradient: 32.48445631153832\n",
      "\n",
      "Weights: [ 6.23648474e-01  1.23553738e-01 -5.58451200e-02 -3.22439085e-02\n",
      " -1.08790563e-02  3.27438292e-01  3.12508812e+00  4.00505046e-02\n",
      " -3.30246173e+01]\n",
      "\n",
      "Accuracy Score: 0.7352941176470589\n",
      "Maximum Accuracy achieved: 0.7352941176470589\n",
      "Momentum value: 0.30000000000000004\n",
      "-----------------------------------------------------------------------------------\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 26.782998249740867\n",
      "\n",
      "Weights: [ 5.09568228e-01  1.33348315e-01 -3.03229456e-02 -1.92632488e-02\n",
      "  2.30622860e-02  2.48470640e-01  2.54852060e+00  3.86430864e-02\n",
      " -2.52745627e+01]\n",
      "\n",
      "Accuracy Score: 0.6176470588235294\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "5000000 Iterations\n",
      "Norm of gradient: 25.682336219450733\n",
      "\n",
      "Weights: [ 4.95903284e-01  1.33094405e-01 -2.63080692e-02 -1.96862068e-02\n",
      "  2.18927816e-02  2.71950575e-01  2.53386154e+00  4.28008088e-02\n",
      " -2.65017363e+01]\n",
      "\n",
      "Accuracy Score: 0.6176470588235294\n",
      "Maximum Accuracy achieved: 0.6176470588235294\n",
      "Momentum value: 0.4\n",
      "-----------------------------------------------------------------------------------\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 20.0846691415053\n",
      "\n",
      "Weights: [ 3.90444839e-01  1.04187330e-01 -2.25287142e-02 -1.51181683e-02\n",
      "  1.71205936e-02  2.07591056e-01  2.02107269e+00  3.29022646e-02\n",
      " -2.04839771e+01]\n",
      "\n",
      "Accuracy Score: 0.6176470588235294\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "5000000 Iterations\n",
      "Norm of gradient: 19.679022393115858\n",
      "\n",
      "Weights: [ 3.85721717e-01  1.04182279e-01 -2.11284995e-02 -1.51544987e-02\n",
      "  1.66725677e-02  2.15313870e-01  2.02706964e+00  3.43485267e-02\n",
      " -2.09140917e+01]\n",
      "\n",
      "Accuracy Score: 0.6176470588235294\n",
      "Maximum Accuracy achieved: 0.6176470588235294\n",
      "Momentum value: 0.5\n",
      "-----------------------------------------------------------------------------------\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 14.29125190933379\n",
      "\n",
      "Weights: [ 2.91409032e-01  6.02331910e-02 -2.73683611e-02 -1.42673903e-02\n",
      " -5.40920891e-03  1.59881751e-01  1.58438154e+00  2.08009005e-02\n",
      " -1.59705060e+01]\n",
      "\n",
      "Accuracy Score: 0.75\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "5000000 Iterations\n",
      "Norm of gradient: 14.178887150530036\n",
      "\n",
      "Weights: [ 2.90316260e-01  6.04346415e-02 -2.69216069e-02 -1.42286232e-02\n",
      " -5.42999086e-03  1.61806821e-01  1.58905797e+00  2.12001209e-02\n",
      " -1.60842134e+01]\n",
      "\n",
      "Accuracy Score: 0.75\n",
      "Maximum Accuracy achieved: 0.75\n",
      "Momentum value: 0.6000000000000001\n",
      "-----------------------------------------------------------------------------------\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 8.772520653021083\n",
      "\n",
      "Weights: [ 2.09465283e-01  5.69927460e-02 -1.35006057e-02 -6.97695554e-03\n",
      "  6.94045945e-03  1.24270276e-01  1.20651707e+00  1.84356691e-02\n",
      " -1.19012517e+01]\n",
      "\n",
      "Accuracy Score: 0.7205882352941176\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "5000000 Iterations\n",
      "Norm of gradient: 8.747108811415812\n",
      "\n",
      "Weights: [ 2.09304951e-01  5.70107357e-02 -1.34325182e-02 -6.97254693e-03\n",
      "  6.91085333e-03  1.24627561e-01  1.20803999e+00  1.85010172e-02\n",
      " -1.19250929e+01]\n",
      "\n",
      "Accuracy Score: 0.7352941176470589\n",
      "Maximum Accuracy achieved: 0.7352941176470589\n",
      "Momentum value: 0.7000000000000001\n",
      "-----------------------------------------------------------------------------------\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 1.2298144054049178\n",
      "\n",
      "Weights: [ 1.46768038e-01  3.76636223e-02 -1.16138035e-02 -3.63674327e-03\n",
      "  3.95010360e-04  9.10626826e-02  8.92886062e-01  1.11846694e-02\n",
      " -8.63794429e+00]\n",
      "\n",
      "Accuracy Score: 0.75\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "5000000 Iterations\n",
      "Norm of gradient: 1.2047223926581998\n",
      "\n",
      "Weights: [ 1.46774414e-01  3.76660389e-02 -1.16013108e-02 -3.63882197e-03\n",
      "  3.73530082e-04  9.11686662e-02  8.93601483e-01  1.11985574e-02\n",
      " -8.64609822e+00]\n",
      "\n",
      "Accuracy Score: 0.7647058823529411\n",
      "Maximum Accuracy achieved: 0.7647058823529411\n",
      "Momentum value: 0.8\n",
      "-----------------------------------------------------------------------------------\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 0.00010056040643521037\n",
      "\n",
      "Weights: [ 1.45316976e-01  3.64848629e-02 -1.19374178e-02 -3.64748664e-03\n",
      " -4.91435178e-04  8.97480761e-02  8.82337585e-01  1.07231320e-02\n",
      " -8.52953814e+00]\n",
      "\n",
      "Accuracy Score: 0.75\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "1001160 Iterations\n",
      "Norm of gradient: 9.999973937948724e-05\n",
      "\n",
      "Weights: [ 1.45318185e-01  3.64855124e-02 -1.19370892e-02 -3.64756770e-03\n",
      " -4.91482651e-04  8.97509451e-02  8.82358915e-01  1.07236563e-02\n",
      " -8.52976979e+00]\n",
      "\n",
      "Accuracy Score: 0.75\n",
      "Maximum Accuracy achieved: 0.75\n",
      "Momentum value: 0.9\n",
      "-----------------------------------------------------------------------------------\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "1000000 Iterations\n",
      "Norm of gradient: 0.0001005491843685467\n",
      "\n",
      "Weights: [ 1.45317005e-01  3.64848787e-02 -1.19374098e-02 -3.64748861e-03\n",
      " -4.91436334e-04  8.97481459e-02  8.82338104e-01  1.07231448e-02\n",
      " -8.52954378e+00]\n",
      "\n",
      "Accuracy Score: 0.75\n",
      "LEARNING RATE: 0.002 \n",
      "\n",
      "1001137 Iterations\n",
      "Norm of gradient: 9.999965186123162e-05\n",
      "\n",
      "Weights: [ 1.45318190e-01  3.64855153e-02 -1.19370877e-02 -3.64756806e-03\n",
      " -4.91482863e-04  8.97509579e-02  8.82359010e-01  1.07236587e-02\n",
      " -8.52977082e+00]\n",
      "\n",
      "Accuracy Score: 0.75\n",
      "Maximum Accuracy achieved: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Best set of parameters so far:\n",
    "m = [1e6, 5e6]\n",
    "l = [2e-3]\n",
    "\n",
    "for i in np.arange(0,1,0.1):\n",
    "    print(\"Momentum value: \" + str(i))\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    optimize_Momentum(data=train, max_iter=m, learning_rate=l, pred_data=test, momentum = i)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
