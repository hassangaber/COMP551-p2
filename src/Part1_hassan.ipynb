{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00ee38e",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "## These are just preliminary tests for now (10/10/2021 4pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b19e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Importing standard modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import List\n",
    "import lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f84598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/diabetes/diabetes_train.csv')\n",
    "test = pd.read_csv('../data/diabetes/diabetes_test.csv')\n",
    "valid = pd.read_csv('../data/diabetes/diabetes_val.csv')\n",
    "all_data = pd.concat([train, test, valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec35b11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminated after 100000 iterations, with norm of the gradient equal to 36.815778107561414\n",
      "the weight found: [ 4.91495421e+01  4.23940272e+00 -9.58402278e+00 -9.93080751e-02\n",
      " -1.76813846e+00  7.52626298e-01  6.63346303e+01 -4.18990007e+00\n",
      " -3.42566248e+02]\n"
     ]
    }
   ],
   "source": [
    "# Test run with logistic regression model\n",
    "x=all_data.iloc[:,:-1].to_numpy()\n",
    "y=all_data.iloc[:,-1:]\n",
    "yi=y.to_numpy().ravel()\n",
    "\n",
    "model = lr.LogisticRegression(verbose=True, )\n",
    "yh = model.fit(x,yi).predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e2b0bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate:  0.1\n",
      "Maximum Iterations:  10000.0\n",
      "terminated after 10000 iterations, with norm of the gradient equal to 65.07728899724842\n",
      "the weight found: [ 55.27367567   6.67249685  -8.4858123   -1.74922103   3.14995522\n",
      "  -1.53885071   6.47275861  -3.39637402 -35.68441955]\n",
      "Maximum Iterations:  90000.0\n",
      "terminated after 90000 iterations, with norm of the gradient equal to 67.54828043093943\n",
      "the weight found: [  54.98527949    6.54762012   -8.59805394   -1.74599558    2.72280474\n",
      "    1.95814347   50.53100348   -3.34895796 -294.04107342]\n",
      "Maximum Iterations:  100000.0\n",
      "terminated after 100000 iterations, with norm of the gradient equal to 66.49749138475383\n",
      "the weight found: [  55.77546839    7.10427188   -8.34629678   -1.56957379    2.78822622\n",
      "    2.42073196   55.22146839   -3.76922089 -323.93717164]\n",
      "Learning Rate:  1\n",
      "Maximum Iterations:  10000.0\n",
      "terminated after 10000 iterations, with norm of the gradient equal to 65.152916286248\n",
      "the weight found: [ 552.56185586   66.69801781  -84.94393443  -17.45478527   31.40153529\n",
      "  -15.87212293   65.08928087  -32.98846092 -356.7629986 ]\n",
      "Maximum Iterations:  90000.0\n",
      "terminated after 90000 iterations, with norm of the gradient equal to 65.95849688769455\n",
      "the weight found: [  547.56259056    70.30732484   -82.28906323   -15.74029144\n",
      "    28.74453229    21.50393351   498.98179741   -32.68787058\n",
      " -2937.62369501]\n",
      "Maximum Iterations:  100000.0\n",
      "terminated after 100000 iterations, with norm of the gradient equal to 67.52409783298762\n",
      "the weight found: [  555.44514893    67.70052703   -85.56748404   -15.38399717\n",
      "    28.06855758    23.2934108    546.00984277   -35.02033916\n",
      " -3235.99921801]\n",
      "Learning Rate:  10\n",
      "Maximum Iterations:  10000.0\n",
      "terminated after 10000 iterations, with norm of the gradient equal to 67.29620775419907\n",
      "the weight found: [ 5518.69628758   546.26553743  -938.073913    -196.20397179\n",
      "   284.52627862  -191.38305369   647.88488909  -369.31146234\n",
      " -3570.06458528]\n",
      "Maximum Iterations:  90000.0\n",
      "terminated after 90000 iterations, with norm of the gradient equal to 66.31994418549085\n",
      "the weight found: [  5476.47290646    695.93407958   -827.06962921   -158.81974417\n",
      "    286.35330103    210.78786432   4974.4727453    -324.98063874\n",
      " -29367.24413724]\n",
      "Maximum Iterations:  100000.0\n",
      "terminated after 100000 iterations, with norm of the gradient equal to 65.95849688769455\n",
      "the weight found: [  5551.49666732    720.35926431   -818.46919688   -167.94653019\n",
      "    292.56153565    245.7626186    5447.94261245   -350.2540668\n",
      " -32348.53611633]\n",
      "Learning Rate:  0.001\n",
      "Maximum Iterations:  10000.0\n",
      "terminated after 10000 iterations, with norm of the gradient equal to 65.27604148281094\n",
      "the weight found: [ 0.60725578  0.06682714 -0.09697304 -0.02115336  0.03347322 -0.02011823\n",
      "  0.05880928 -0.04159531 -0.35273721]\n",
      "Maximum Iterations:  90000.0\n",
      "terminated after 90000 iterations, with norm of the gradient equal to 65.02853125151216\n",
      "the weight found: [ 6.32591980e-01  7.68834200e-02 -8.92396257e-02 -2.33270904e-02\n",
      "  3.45164149e-02  9.16621424e-04  4.99563092e-01 -4.02898071e-02\n",
      " -3.02661397e+00]\n",
      "Maximum Iterations:  100000.0\n",
      "terminated after 100000 iterations, with norm of the gradient equal to 65.00015626788687\n",
      "the weight found: [ 0.63260215  0.0780354  -0.08835453 -0.02346052  0.03457066  0.00354448\n",
      "  0.54864778 -0.03971575 -3.34298196]\n"
     ]
    }
   ],
   "source": [
    "# Function to find convergent solution of gradient descent as a function of learning-rate and maximum iterations\n",
    "\n",
    "def optimize(max_iter, learning_rate, train_data, pred_data):\n",
    "    \n",
    "    # Input data, features and binary labels column\n",
    "    Xin=train_data.iloc[:,:-1].to_numpy()\n",
    "    Yin=train_data.iloc[:,-1:].to_numpy().ravel()\n",
    "    Xp=pred_data.iloc[:,:-1].to_numpy()\n",
    "    Yp=pred_data.iloc[:,-1:].to_numpy().ravel()\n",
    "    \n",
    "    T = []\n",
    "    \n",
    "    # Iterate through the input parameters\n",
    "    for l in learning_rate:\n",
    "        print(\"Learning Rate: \", l)\n",
    "        for m in max_iter:\n",
    "            print(\"Maximum Iterations: \", m)\n",
    "            model = lr.LogisticRegression(verbose=True, add_bias=True, learning_rate=l, max_iters=m)\n",
    "            yh = model.fit(Xin,Yin).predict(Xp)\n",
    "            T.append(yh)\n",
    "        \n",
    "        #print(accuracy_score(T,Yp))\n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "\n",
    "mi = [1e4,9e4,1e5]\n",
    "learn = [.1,1,10,0.001]\n",
    "\n",
    "optimize(max_iter=mi, learning_rate=learn, train_data=train, pred_data=valid)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
